{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062bb22d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "062bb22d",
    "outputId": "12effd0d-61be-4dc7-9c90-52fbfd065b39",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes : 5012\n",
      "Nombre de colonnes : 13\n",
      "Types de colonnes :\n",
      " Customer Id            object\n",
      "YearOfObservation       int64\n",
      "Insured_Period        float64\n",
      "Residential             int64\n",
      "Building_Painted       object\n",
      "Building_Fenced        object\n",
      "Garden                 object\n",
      "Settlement             object\n",
      "Building Dimension    float64\n",
      "Building_Type          object\n",
      "NumberOfWindows        object\n",
      "Geo_Code               object\n",
      "Claim                  object\n",
      "dtype: object\n",
      "Valeurs manquantes :\n",
      " Customer Id            0\n",
      "YearOfObservation      0\n",
      "Insured_Period         0\n",
      "Residential            0\n",
      "Building_Painted       0\n",
      "Building_Fenced        0\n",
      "Garden                 4\n",
      "Settlement             0\n",
      "Building Dimension    77\n",
      "Building_Type          0\n",
      "NumberOfWindows        0\n",
      "Geo_Code              73\n",
      "Claim                  0\n",
      "dtype: int64\n",
      "Liste des colonnes :\n",
      "['Customer Id', 'YearOfObservation', 'Insured_Period', 'Residential', 'Building_Painted', 'Building_Fenced', 'Garden', 'Settlement', 'Building Dimension', 'Building_Type', 'NumberOfWindows', 'Geo_Code', 'Claim']\n",
      "Liste des colonnes numériques :\n",
      "['YearOfObservation', 'Insured_Period', 'Residential', 'Building Dimension']\n",
      "Liste des attributs discrets :\n",
      "['Customer Id', 'Building_Painted', 'Building_Fenced', 'Garden', 'Settlement', 'Building_Type', 'NumberOfWindows', 'Geo_Code', 'Claim']\n",
      "Statistiques descriptives :\n",
      "        Customer Id  YearOfObservation  Insured_Period  Residential  \\\n",
      "count         5012        5012.000000     5012.000000  5012.000000   \n",
      "unique        5012                NaN             NaN          NaN   \n",
      "top         H13501                NaN             NaN          NaN   \n",
      "freq             1                NaN             NaN          NaN   \n",
      "mean           NaN        2013.660215        0.869713     0.301077   \n",
      "std            NaN           1.383134        0.219496     0.458772   \n",
      "min            NaN        2012.000000        0.500000     0.000000   \n",
      "25%            NaN        2012.000000        0.500000     0.000000   \n",
      "50%            NaN        2013.000000        1.000000     0.000000   \n",
      "75%            NaN        2015.000000        1.000000     1.000000   \n",
      "max            NaN        2016.000000        1.000000     1.000000   \n",
      "\n",
      "       Building_Painted Building_Fenced Garden Settlement  Building Dimension  \\\n",
      "count              5012            5012   5008       5012         4935.000000   \n",
      "unique                2               2      2          2                 NaN   \n",
      "top                   V               N      O          R                 NaN   \n",
      "freq               3763            2535   2532       2537                 NaN   \n",
      "mean                NaN             NaN    NaN        NaN         1876.898683   \n",
      "std                 NaN             NaN    NaN        NaN         2267.277397   \n",
      "min                 NaN             NaN    NaN        NaN            1.000000   \n",
      "25%                 NaN             NaN    NaN        NaN          520.000000   \n",
      "50%                 NaN             NaN    NaN        NaN         1067.000000   \n",
      "75%                 NaN             NaN    NaN        NaN         2280.000000   \n",
      "max                 NaN             NaN    NaN        NaN        20840.000000   \n",
      "\n",
      "          Building_Type NumberOfWindows Geo_Code Claim  \n",
      "count              5012            5012     4939  5012  \n",
      "unique                4              11     1115     2  \n",
      "top     Non-combustible         without     6088   non  \n",
      "freq               2310            2476      102  3886  \n",
      "mean                NaN             NaN      NaN   NaN  \n",
      "std                 NaN             NaN      NaN   NaN  \n",
      "min                 NaN             NaN      NaN   NaN  \n",
      "25%                 NaN             NaN      NaN   NaN  \n",
      "50%                 NaN             NaN      NaN   NaN  \n",
      "75%                 NaN             NaN      NaN   NaN  \n",
      "max                 NaN             NaN      NaN   NaN  \n",
      "Statistiques descriptives (attributs numériques) :\n",
      "       YearOfObservation  Insured_Period  Residential  Building Dimension\n",
      "count        5012.000000     5012.000000  5012.000000         4935.000000\n",
      "mean         2013.660215        0.869713     0.301077         1876.898683\n",
      "std             1.383134        0.219496     0.458772         2267.277397\n",
      "min          2012.000000        0.500000     0.000000            1.000000\n",
      "25%          2012.000000        0.500000     0.000000          520.000000\n",
      "50%          2013.000000        1.000000     0.000000         1067.000000\n",
      "75%          2015.000000        1.000000     1.000000         2280.000000\n",
      "max          2016.000000        1.000000     1.000000        20840.000000\n",
      "Statistiques descriptives (tous les attributs) :\n",
      "       Customer Id  YearOfObservation  Insured_Period  Residential  \\\n",
      "count         5012        5012.000000     5012.000000  5012.000000   \n",
      "unique        5012                NaN             NaN          NaN   \n",
      "top         H13501                NaN             NaN          NaN   \n",
      "freq             1                NaN             NaN          NaN   \n",
      "mean           NaN        2013.660215        0.869713     0.301077   \n",
      "std            NaN           1.383134        0.219496     0.458772   \n",
      "min            NaN        2012.000000        0.500000     0.000000   \n",
      "25%            NaN        2012.000000        0.500000     0.000000   \n",
      "50%            NaN        2013.000000        1.000000     0.000000   \n",
      "75%            NaN        2015.000000        1.000000     1.000000   \n",
      "max            NaN        2016.000000        1.000000     1.000000   \n",
      "\n",
      "       Building_Painted Building_Fenced Garden Settlement  Building Dimension  \\\n",
      "count              5012            5012   5008       5012         4935.000000   \n",
      "unique                2               2      2          2                 NaN   \n",
      "top                   V               N      O          R                 NaN   \n",
      "freq               3763            2535   2532       2537                 NaN   \n",
      "mean                NaN             NaN    NaN        NaN         1876.898683   \n",
      "std                 NaN             NaN    NaN        NaN         2267.277397   \n",
      "min                 NaN             NaN    NaN        NaN            1.000000   \n",
      "25%                 NaN             NaN    NaN        NaN          520.000000   \n",
      "50%                 NaN             NaN    NaN        NaN         1067.000000   \n",
      "75%                 NaN             NaN    NaN        NaN         2280.000000   \n",
      "max                 NaN             NaN    NaN        NaN        20840.000000   \n",
      "\n",
      "          Building_Type NumberOfWindows Geo_Code Claim  \n",
      "count              5012            5012     4939  5012  \n",
      "unique                4              11     1115     2  \n",
      "top     Non-combustible         without     6088   non  \n",
      "freq               2310            2476      102  3886  \n",
      "mean                NaN             NaN      NaN   NaN  \n",
      "std                 NaN             NaN      NaN   NaN  \n",
      "min                 NaN             NaN      NaN   NaN  \n",
      "25%                 NaN             NaN      NaN   NaN  \n",
      "50%                 NaN             NaN      NaN   NaN  \n",
      "75%                 NaN             NaN      NaN   NaN  \n",
      "max                 NaN             NaN      NaN   NaN  \n",
      "Valeurs manquantes par attribut :\n",
      "Customer Id            0\n",
      "YearOfObservation      0\n",
      "Insured_Period         0\n",
      "Residential            0\n",
      "Building_Painted       0\n",
      "Building_Fenced        0\n",
      "Garden                 4\n",
      "Settlement             0\n",
      "Building Dimension    77\n",
      "Building_Type          0\n",
      "NumberOfWindows        0\n",
      "Geo_Code              73\n",
      "Claim                  0\n",
      "dtype: int64\n",
      "Nombre total de valeurs manquantes :\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Charger les données\n",
    "train_data = pd.read_csv('train_Insurance.csv')\n",
    "test_data = pd.read_csv('test_Insurance.csv')\n",
    "\n",
    "# Analyse exploratoire des données\n",
    "print(\"Nombre de lignes :\", len(train_data))\n",
    "print(\"Nombre de colonnes :\", len(train_data.columns))\n",
    "print(\"Types de colonnes :\\n\", train_data.dtypes)\n",
    "print(\"Valeurs manquantes :\\n\", train_data.isna().sum())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exploration initiale\n",
    "L = list(train_data.columns)\n",
    "L_num = list(train_data.select_dtypes(exclude='object').columns)\n",
    "L_d = list(train_data.select_dtypes(include='object').columns)\n",
    "print(\"Liste des colonnes :\")\n",
    "print(L)\n",
    "print(\"Liste des colonnes numériques :\")\n",
    "print(L_num)\n",
    "print(\"Liste des attributs discrets :\")\n",
    "print(L_d)\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"Statistiques descriptives :\\n\", train_data.describe(include=\"all\"))\n",
    "descriptives_numeric = train_data.describe()\n",
    "descriptives_all = train_data.describe(include='all')\n",
    "print(\"Statistiques descriptives (attributs numériques) :\")\n",
    "print(descriptives_numeric)\n",
    "print(\"Statistiques descriptives (tous les attributs) :\")\n",
    "print(descriptives_all)\n",
    "\n",
    "# Identification des valeurs manquantes\n",
    "missing_values = train_data.isnull().sum()\n",
    "total_missing = train_data.isnull().sum().sum()\n",
    "print(\"Valeurs manquantes par attribut :\")\n",
    "print(missing_values)\n",
    "print(\"Nombre total de valeurs manquantes :\")\n",
    "print(total_missing)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4490ce6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a copy of the original dataframe\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df_copy \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcols\u001b[49m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Calcul des quartiles et de l'écart interquartile\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     Q1, Q3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(df_copy[col], [\u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m75\u001b[39m])\n\u001b[0;32m      7\u001b[0m     IQR \u001b[38;5;241m=\u001b[39m Q3 \u001b[38;5;241m-\u001b[39m Q1\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cols' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a copy of the original dataframe\n",
    "df_copy = train_data.copy()\n",
    "\n",
    "for col in cols:\n",
    "    # Calcul des quartiles et de l'écart interquartile\n",
    "    Q1, Q3 = np.percentile(df_copy[col], [25, 75])\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Calcul des limites pour les valeurs aberrantes\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "\n",
    "    # Identification des outliers\n",
    "    outliers = df_copy[(df_copy[col] < lower_limit) | (df_copy[col] > upper_limit)]\n",
    "    \n",
    "    print(f\"Nombre d'outliers pour {col} : {len(outliers)}\")\n",
    "\n",
    "    # Remplacement des valeurs aberrantes par les limites\n",
    "    df_copy[col] = np.where(df_copy[col] >= upper_limit, upper_limit,\n",
    "                       np.where(df_copy[col] <= lower_limit, lower_limit, df_copy[col]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Sélection des colonnes numériques\n",
    "numeric_columns = ['YearOfObservation', 'Insured_Period', 'Residential', 'Building Dimension']\n",
    "\n",
    "# Création de boxplots pour chaque colonne numérique\n",
    "for column in numeric_columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=train_data[column])\n",
    "    plt.title(f'Boxplot de {column}')\n",
    "    plt.show()\n",
    "\n",
    "    # Calcul des limites pour les valeurs aberrantes\n",
    "    Q1, Q3 = np.percentile(train_data[column], [25, 75])\n",
    "    IQR = Q3 - Q1\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "\n",
    "    # Identification des valeurs aberrantes\n",
    "    outliers = train_data[column][(train_data[column] > upper_limit) | (train_data[column] < lower_limit)]\n",
    "    print(outliers)\n",
    "    # Affichage du nombre d'outliers\n",
    "    print(f\"Nombre d'outliers pour {column} : {len(outliers)}\")\n",
    "   \n",
    "\n",
    "\n",
    "    # Traitement des valeurs aberrantes\n",
    "    train_data[column] = np.where(train_data[column] >= upper_limit, upper_limit,\n",
    "                          np.where(train_data[column] <= lower_limit, lower_limit, train_data[column]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71c285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2e3eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5c2e3eb",
    "outputId": "cc059092-d04a-4abb-89c2-28dd559aa545"
   },
   "outputs": [],
   "source": [
    "# Justifications:\n",
    "# Pour \"Garden\", la perte d'informations en supprimant les lignes est négligeable.\n",
    "# Pour \"Building Dimension\", utiliser la médiane est approprié pour éviter l'influence des valeurs extrêmes.\n",
    "# Pour \"Geo_Code\", remplacer par le mode est une option raisonnable étant donné que c'est une variable catégorielle avec relativement peu de modalités manquantes.\n",
    "\n",
    "\n",
    "\n",
    "# Suppression des instances ayant des valeurs manquantes dans la colonne 'Garden'\n",
    "#la perte d'information en supprimant des lignes\n",
    "train_data = train_data.dropna(subset=['Garden'])\n",
    "\n",
    "# Remplissage des valeurs manquantes dans 'Building Dimension' par la médiane\n",
    "# La médiane est moins sensible aux valeurs aberrantes que la moyenne.\n",
    "# Si votre colonne 'Building Dimension' contient des valeurs extrêmes,\n",
    "# l'utilisation de la médiane pour remplir les valeurs manquantes peut donner de meilleurs résultats.\n",
    "\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "train_data['Building Dimension'] = median_imputer.fit_transform(train_data['Building Dimension'].values.reshape(-1,1))\n",
    "\n",
    "# Cette stratégie est souvent utilisée pour les variables catégorielles.\n",
    "# L’idée est de remplir les valeurs manquantes par la valeur la plus fréquente.\n",
    "# Cela peut être une bonne stratégie lorsque la variable a une catégorie dominante.\n",
    "# Remplissage des valeurs manquantes dans 'Geo_Code' par l’élément le plus fréquent\n",
    "freq_imputer = SimpleImputer(strategy='most_frequent')\n",
    "train_data['Geo_Code'] = freq_imputer.fit_transform(train_data['Geo_Code'].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "#  test de nouveau  des valeurs manquantes\n",
    "missing_values = train_data.isnull().sum()\n",
    "total_missing = train_data.isnull().sum().sum()\n",
    "print(\"Valeurs manquantes par attribut :\")\n",
    "print(missing_values)\n",
    "print(\"Nombre total de valeurs manquantes :\")\n",
    "print(total_missing)\n",
    "\n",
    "\n",
    "# Suppression des instances ayant des valeurs manquantes dans la colonne 'Garden'\n",
    "test_data = test_data.dropna(subset=['Garden'])\n",
    "\n",
    "# Remplissage des valeurs manquantes dans 'Building Dimension' par la médiane\n",
    "# Notez que nous utilisons 'transform' au lieu de 'fit_transform' pour utiliser la médiane calculée sur les données d'entraînement\n",
    "test_data['Building Dimension'] = median_imputer.transform(test_data['Building Dimension'].values.reshape(-1,1))\n",
    "\n",
    "# Remplissage des valeurs manquantes dans 'Geo_Code' par l’élément le plus fréquent\n",
    "# Encore une fois, nous utilisons 'transform' pour utiliser le mode calculé sur les données d'entraînement\n",
    "test_data['Geo_Code'] = freq_imputer.transform(test_data['Geo_Code'].values.reshape(-1,1))\n",
    "\n",
    "# Identification des valeurs manquantes\n",
    "missing_values = test_data.isnull().sum()\n",
    "total_missing = test_data.isnull().sum().sum()\n",
    "print(\"Valeurs manquantes par attribut :\")\n",
    "print(missing_values)\n",
    "print(\"Nombre total de valeurs manquantes :\")\n",
    "print(total_missing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d6371a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f0d6371a",
    "outputId": "db21816c-5812-4ba4-bbaf-cbbafda5c20f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0de4dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1e0de4dc",
    "outputId": "67c39554-3657-41af-c374-d037c972f3de"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Sélection des colonnes numériques\n",
    "numeric_columns = ['YearOfObservation', 'Insured_Period', 'Residential', 'Building Dimension']\n",
    "\n",
    "# Création de boxplots pour chaque colonne numérique\n",
    "for column in numeric_columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=train_data[column])\n",
    "    plt.title(f'Boxplot de {column}')\n",
    "    plt.show()\n",
    "\n",
    "    # Calcul des limites pour les valeurs aberrantes\n",
    "    Q1, Q3 = np.percentile(train_data[column], [25, 75])\n",
    "    IQR = Q3 - Q1\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "\n",
    "    # Identification des valeurs aberrantes\n",
    "    outliers = train_data[column][(train_data[column] > upper_limit) | (train_data[column] < lower_limit)]\n",
    "    # Affichage du nombre d'outliers\n",
    "    print(f\"Nombre d'outliers pour {column} : {len(outliers)}\")\n",
    "   \n",
    "\n",
    "\n",
    "    # Traitement des valeurs aberrantes\n",
    "    train_data[column] = np.where(train_data[column] >= upper_limit, upper_limit,\n",
    "                          np.where(train_data[column] <= lower_limit, lower_limit, train_data[column]))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Sélection des colonnes numériques\n",
    "numeric_columns = ['YearOfObservation', 'Insured_Period', 'Residential', 'Building Dimension']\n",
    "\n",
    "# Création de boxplots pour chaque colonne numérique\n",
    "for column in numeric_columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=test_data[column])\n",
    "    plt.title(f'Boxplot de {column}')\n",
    "    plt.show()\n",
    "\n",
    "    # Calcul des limites pour les valeurs aberrantes\n",
    "    Q1, Q3 = np.percentile(test_data[column], [25, 75])\n",
    "    IQR = Q3 - Q1\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "\n",
    "    # Identification des valeurs aberrantes\n",
    "    outliers = test_data[column][(test_data[column] > upper_limit) | (test_data[column] < lower_limit)]\n",
    "\n",
    "    \n",
    "    # Affichage du nombre d'outliers\n",
    "    print(f\"Nombre d'outliers pour {column} : {len(outliers)}\")\n",
    "    \n",
    "    # Traitement des valeurs aberrantes\n",
    "    test_data[column] = np.where(test_data[column] >= upper_limit, upper_limit,\n",
    "                          np.where(test_data[column] <= lower_limit, lower_limit, test_data[column]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b06bf7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7b06bf7",
    "outputId": "d3e875ba-e003-44b8-b039-4d5937d47189"
   },
   "outputs": [],
   "source": [
    "print(train_data[train_data.duplicated()])\n",
    "\n",
    "print(test_data[test_data.duplicated()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049202c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "049202c7",
    "outputId": "faa4fa72-0342-4771-d84d-a0261219462e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Liste des colonnes pour lesquelles vous voulez créer des KDE\n",
    "cols = ['YearOfObservation', 'Insured_Period', 'Building Dimension']\n",
    "\n",
    "for col in cols:\n",
    "    # Calcul de la moyenne\n",
    "    moyenne = train_data[col].mean()\n",
    "    print(\"Moyenne de \" + col + \" :\", moyenne)\n",
    "\n",
    "    # Calcul de la médiane\n",
    "    median = train_data[col].median()\n",
    "    print(\"Médiane de \" + col + \" :\", median)\n",
    "\n",
    "    # Calcul du mode\n",
    "    train_data.sort_values(by=col, inplace=True)\n",
    "    x_vals = train_data[col].values\n",
    "    kde = gaussian_kde(x_vals)\n",
    "    y_vals = kde(x_vals)\n",
    "    valeur_mode = x_vals[np.argmax(y_vals)]\n",
    "    print(\"Mode de \" + col + \" :\", valeur_mode)\n",
    "\n",
    "   # Ajouter des lignes verticales pour la moyenne, la médiane et le mode\n",
    "    plt.axvline(moyenne, color='r', linestyle='--', label='Moyenne')\n",
    "    plt.axvline(median, color='g', linestyle='-', label='Médiane')\n",
    "    plt.axvline(valeur_mode, color='b', linestyle='-', label='Mode')\n",
    "\n",
    "    # Créer un histogramme pour la colonne\n",
    "    sns.histplot(x=col, data=train_data, bins=3, kde=True, stat='density')\n",
    "    plt.title(\"Histogramme avec KDE de \" + col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Densité de probabilité\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c1a5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "625c1a5d",
    "outputId": "f5cfb205-7ca9-451d-d127-5a08e1813ec5"
   },
   "outputs": [],
   "source": [
    "    #le coefficient d’asymétrie vous aide à comprendre la distribution de vos données et peut vous donner des indications sur la manière\n",
    "#de prétraiter vos données pour l’analyse.\n",
    "\n",
    "\n",
    "# YearOfObservation : Le coefficient d’asymétrie est de 0.35, ce qui indique une légère asymétrie à droite.\n",
    "# Cela signifie que la queue de la distribution est légèrement étirée vers les valeurs plus grandes que la médiane.\n",
    "\n",
    "# Insured_Period : Le coefficient d’asymétrie est de -1.09, ce qui indique une asymétrie à gauche.\n",
    "# Cela signifie que la queue de la distribution est étirée vers les valeurs plus petites que la médiane.\n",
    "\n",
    "# Building Dimension : Le coefficient d’asymétrie est de 1.13, ce qui indique une asymétrie à droite.\n",
    "# Cela signifie que la queue de la distribution est étirée vers les valeurs plus grandes que la médiane.\n",
    "\n",
    "\n",
    "\n",
    "# Liste des colonnes pour lesquelles vous voulez calculer le coefficient d'asymétrie\n",
    "cols = ['YearOfObservation', 'Insured_Period', 'Building Dimension']\n",
    "\n",
    "for col in cols:\n",
    "    # Calcul du coefficient d'asymétrie\n",
    "    skewness = train_data[col].skew()\n",
    "    print(\"Coefficient d'asymétrie pour \" + col + \" :\", skewness)\n",
    "\n",
    "    # Interprétation du coefficient d'asymétrie\n",
    "    if skewness > 0:\n",
    "        print(\"La distribution de \" + col + \" est asymétrique à droite.\")\n",
    "    elif skewness < 0:\n",
    "        print(\"La distribution de \" + col + \" est asymétrique à gauche.\")\n",
    "    else:\n",
    "        print(\"La distribution de \" + col + \" est symétrique.\")\n",
    "\n",
    "\n",
    "# Liste des colonnes pour lesquelles vous voulez calculer le coefficient d'asymétrie\n",
    "cols = ['YearOfObservation', 'Insured_Period', 'Building Dimension']\n",
    "\n",
    "for col in cols:\n",
    "    # Calcul du coefficient d'asymétrie\n",
    "    skewness = test_data[col].skew()\n",
    "    print(\"Coefficient d'asymétrie pour \" + col + \" :\", skewness)\n",
    "\n",
    "    # Interprétation du coefficient d'asymétrie\n",
    "    if skewness > 0:\n",
    "        print(\"La distribution de \" + col + \" est asymétrique à droite.\")\n",
    "    elif skewness < 0:\n",
    "        print(\"La distribution de \" + col + \" est asymétrique à gauche.\")\n",
    "    else:\n",
    "        print(\"La distribution de \" + col + \" est symétrique.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90f109",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1d90f109",
    "outputId": "8508d900-1573-44cf-a159-89da1e7f086f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Liste des colonnes pour lesquelles vous voulez créer des histogrammes\n",
    "cols = ['YearOfObservation', 'Residential', 'Building_Painted', 'Building_Fenced', 'Garden', 'Settlement', 'Building_Type', 'NumberOfWindows', 'Geo_Code', 'Claim']\n",
    "\n",
    "for col in cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=col, data=train_data)\n",
    "    plt.title('Histogramme de ' + col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Fréquence')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b4a83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "531b4a83",
    "outputId": "7a535196-2528-44e5-c8c7-3abf02bcf0d7"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Claim', data=train_data, hue ='YearOfObservation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac5ff2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "50ac5ff2",
    "outputId": "17d8bf02-228e-4c09-8c79-57b48912846d"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Claim', data=train_data, hue ='Settlement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6bd453",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "5b6bd453",
    "outputId": "e024a816-0832-4d32-b850-f9b236c1a331"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x='Building_Painted', hue='Claim', data=train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da6365",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Building_Fenced', hue='Claim', data=train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad7986",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Garden', hue='Claim', data=train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfec189",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Settlement', hue='Claim', data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9668901",
   "metadata": {
    "id": "b9668901"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Les modèles de machine learning ont généralement besoin de travailler avec des données numériques,\n",
    "# donc les variables catégorielles doivent être transformées en nombres avant de pouvoir être utilisées pour l’entraînement d’un modèle.\n",
    "\n",
    "# Création d'une instance de LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Application de l'encodage de label à la colonne 'Claim'\n",
    "train_data['Claim'] = label_encoder.fit_transform(train_data['Claim'])\n",
    "\n",
    "\n",
    "# Application de l'encodage de label à la colonne 'Claim'\n",
    "test_data['Claim'] = label_encoder.fit_transform(test_data['Claim'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f80cc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94f80cc8",
    "outputId": "0460822e-3074-4e71-9d17-d32b766c33ed"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import pandas as pd\n",
    "#Nombre d'outliers pour Building Dimension : 396\n",
    "\n",
    "# Cette méthode est efficace pour minimiser l'impact des valeurs aberrantes,\n",
    "# ce qui pourrait être le cas ici étant donné l'asymétrie.\n",
    "\n",
    "\n",
    "# Pour la standardisation RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "train_data['Building Dimension'] = robust_scaler.fit_transform(train_data[['Building Dimension']])\n",
    "\n",
    "print(train_data)\n",
    "# Affichez les statistiques descriptives des données mises à l'échelle\n",
    "print(train_data.describe())\n",
    "\n",
    "\n",
    "# Pour la standardisation RobustScaler\n",
    "test_data['Building Dimension'] = robust_scaler.fit_transform(test_data[['Building Dimension']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038de4a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "038de4a5",
    "outputId": "59a8ddf1-0903-49eb-e6c0-ac8d3d98e578"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Création d'une instance de LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Transformation des valeurs textuelles en valeurs numériques pour certaines colonnes\n",
    "train_data[\"Building_Painted\"] = label_encoder.fit_transform(train_data[\"Building_Painted\"])\n",
    "train_data[\"Building_Fenced\"] = label_encoder.fit_transform(train_data[\"Building_Fenced\"])\n",
    "train_data[\"Garden\"] = label_encoder.fit_transform(train_data[\"Garden\"])\n",
    "train_data[\"Settlement\"] = label_encoder.fit_transform(train_data[\"Settlement\"])\n",
    "train_data[\"Insured_Period\"] = label_encoder.fit_transform(train_data[\"Insured_Period\"])\n",
    "train_data[\"Building_Type\"] = label_encoder.fit_transform(train_data[\"Building_Type\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Affichage du DataFrame final\n",
    "print(train_data)\n",
    "\n",
    "\n",
    "# Transformation des valeurs textuelles en valeurs numériques pour certaines colonnes\n",
    "test_data[\"Building_Painted\"] = label_encoder.fit_transform(test_data[\"Building_Painted\"])\n",
    "test_data[\"Building_Fenced\"] = label_encoder.fit_transform(test_data[\"Building_Fenced\"])\n",
    "test_data[\"Garden\"] = label_encoder.fit_transform(test_data[\"Garden\"])\n",
    "test_data[\"Settlement\"] = label_encoder.fit_transform(test_data[\"Settlement\"])\n",
    "test_data[\"Insured_Period\"] = label_encoder.fit_transform(test_data[\"Insured_Period\"])\n",
    "test_data[\"Building_Type\"] = label_encoder.fit_transform(test_data[\"Building_Type\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a37a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac2a37a9",
    "outputId": "cd1c5781-af73-481f-f3eb-300cfca21ae8"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "#'YearOfObservation'\n",
    "\"\"\"\n",
    "'YearOfObservation' a un coefficient d'asymétrie de 0.35, indiquant une légère asymétrie à droite.\n",
    "Cependant, il n'y a pas d'outliers dans cette variable.\n",
    "La méthode Equal-Width (Uniforme) est utilisée pour diviser la plage de la variable en intervalles de largeur égale.\n",
    "Cette méthode est appropriée ici car la distribution est presque symétrique et sans outliers.\n",
    "\"\"\"\n",
    "\n",
    "#'Building Dimension'\n",
    "\"\"\"\n",
    "'Building Dimension' a un coefficient d'asymétrie de 1.13, indiquant une asymétrie à droite.\n",
    "Il y a 396 outliers dans cette variable.\n",
    "La méthode Equal-Frequency (Quantile) est utilisée pour diviser la plage de la variable de manière à ce que chaque intervalle contienne le même nombre d'observations.\n",
    "Cette méthode est moins sensible aux valeurs aberrantes et convient donc mieux à cette colonne.\n",
    "\"\"\"\n",
    "\n",
    "# Création d'une instance de KBinsDiscretizer pour une discrétisation Equal-Width\n",
    "discretizer_uniform = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "\n",
    "# Discrétisation Equal-Width pour 'YearOfObservation'\n",
    "train_data['YearOfObservation'] = discretizer_uniform.fit_transform(train_data[['YearOfObservation']])\n",
    "\n",
    "# Création d'une instance de KBinsDiscretizer pour une discrétisation Equal-Frequency\n",
    "discretizer_quantile = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "\n",
    "# Discrétisation Equal-Frequency pour 'Building Dimension'\n",
    "train_data['Building Dimension'] = discretizer_quantile.fit_transform(train_data[['Building Dimension']])\n",
    "\n",
    "\n",
    "print(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3b4a9",
   "metadata": {
    "id": "4bb3b4a9"
   },
   "outputs": [],
   "source": [
    "#Réduction\n",
    "# Suppression de certaines colonnes qui ne seront pas utilisées\n",
    "## Customer Id : Cet attribut semble être un identifiant unique pour chaque client.\n",
    "# En général, nous n’utilisons pas les identifiants dans les modèles de machine learning\n",
    "# car ils n’ont pas de pouvoir prédictif. Vous pouvez donc ignorer cet attribut.\n",
    "\n",
    "train_data.drop(['Customer Id', 'Geo_Code', 'NumberOfWindows'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76037150",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76037150",
    "outputId": "432172f7-b1e7-4221-d74b-2048a00cc735"
   },
   "outputs": [],
   "source": [
    "train_data.corr()['Claim'].sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iz4Rdr1PJ9e1",
   "metadata": {
    "id": "iz4Rdr1PJ9e1"
   },
   "outputs": [],
   "source": [
    "test_data.drop(['Customer Id', 'Geo_Code', 'NumberOfWindows'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd67af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37cd67af",
    "outputId": "b9c89cd9-c90c-4ba8-dbf8-b11223d20ddb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Séparer les caractéristiques et la cible\n",
    "X_train = train_data.drop(['Claim'], axis=1)\n",
    "y_train = train_data['Claim']\n",
    "X_test = test_data.drop(['Claim'], axis=1)\n",
    "y_test = test_data['Claim']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialiser les modèles\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    MLPClassifier()\n",
    "]\n",
    "\n",
    "# Entraîner et évaluer chaque modèle\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    # le modèle fait des prédictions sur X_test et compare ces prédictions à y_test\n",
    "    #pour calculer un score de performance.\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(f'Score for {model.__class__.__name__}: {score}')\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Initialiser les modèles\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    MLPClassifier()\n",
    "]\n",
    "\n",
    "# Entraîner et évaluer chaque modèle\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calcul des métriques de performance\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Affichage des métriques de performance\n",
    "    print(f'Metrics for {model.__class__.__name__}:')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1_score}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd77b869",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "fd77b869",
    "outputId": "f64edcc9-b8ef-4588-e2af-032fd4540fe2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Initialiser les modèles\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(probability=True),\n",
    "    MLPClassifier()\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Entraîner et évaluer chaque modèle\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    # Prédire les probabilités de la classe positive\n",
    "    y_score = model.predict_proba(X_test)[:, 1]\n",
    "    # Calculer les points de la courbe ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    # Calculer l'AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # Afficher la courbe ROC\n",
    "    plt.plot(fpr, tpr, label=f'{model.__class__.__name__} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Afficher la ligne de hasard\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a7180",
   "metadata": {
    "id": "ec0a7180"
   },
   "outputs": [],
   "source": [
    "# Commentaire sur LogisticRegression\n",
    "\"\"\"\n",
    "LogisticRegression a une AUC de 0.68, ce qui est la plus grande parmi tous les modèles.\n",
    "Cela indique qu'il a la meilleure performance et a une plus grande probabilité de classer correctement un individu choisi au hasard.\n",
    "\"\"\"\n",
    "\n",
    "# Commentaire sur DecisionTreeClassifier\n",
    "\"\"\"\n",
    "DecisionTreeClassifier a une AUC de 0.49, ce qui est proche de celle d'un classificateur aléatoire (AUC = 0.5).\n",
    "Il pourrait ne pas être très utile pour faire des prédictions précises.\n",
    "\"\"\"\n",
    "\n",
    "# Commentaire sur SVC\n",
    "\"\"\"\n",
    "SVC a une AUC de 0.50, ce qui est équivalent à celle d'un classificateur aléatoire.\n",
    "Il pourrait ne pas être très utile pour faire des prédictions précises.\n",
    "\"\"\"\n",
    "\n",
    "# Commentaire sur RandomForrestClassifier\n",
    "\"\"\"\n",
    "RandomForrestClassifier a une AUC de 0.53, ce qui est légèrement meilleure que celle d'un classificateur aléatoire,\n",
    "mais il est toujours loin derrière le modèle LogisticRegression.\n",
    "\"\"\"\n",
    "\n",
    "# Commentaire sur MLPClassifier\n",
    "\"\"\"\n",
    "MLPClassifier a la plus faible AUC parmi tous les modèles (AUC = 0.41), ce qui indique qu'il a la pire performance.\n",
    "Il a une plus faible probabilité de classer correctement un individu choisi au hasard.\n",
    "\"\"\"\n",
    "\n",
    "# Résumé\n",
    "\"\"\"\n",
    "En résumé, le modèle LogisticRegression semble être le meilleur choix parmi ces modèles pour ce problème de classification spécifique,\n",
    "selon les courbes ROC et les valeurs d'AUC. Cependant, il est important de noter que ces résultats sont spécifiques à l'ensemble de données\n",
    "sur lequel les modèles ont été formés et testés. Les performances peuvent varier avec différents ensembles de données.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZtD0SB76tfa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "ZtD0SB76tfa1",
    "outputId": "dca973c6-1d57-42c7-fc4f-e2071127be6b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialiser les modèles\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(probability=True),\n",
    "    MLPClassifier()\n",
    "]\n",
    "\n",
    "# Pour chaque modèle\n",
    "for model in models:\n",
    "    # Entraîner le modèle\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédire les probabilités des classes\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "\n",
    "    # Calculer les valeurs de précision et de rappel pour différents seuils\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_scores[:, 1])\n",
    "\n",
    "    # Afficher la courbe de précision-rappel\n",
    "    plt.plot(recall, precision, label=model.__class__.__name__)\n",
    "\n",
    "# Ajouter des détails au graphique\n",
    "plt.xlabel('Rappel')\n",
    "plt.ylabel('Précision')\n",
    "plt.title('Courbe de précision-rappel')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9_rDhdlCviOs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9_rDhdlCviOs",
    "outputId": "8ff108f6-761c-40b3-a48f-3b7c154f1a6c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "# Initialiser les modèles\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(probability=True),\n",
    "    MLPClassifier()\n",
    "]\n",
    "\n",
    "# Pour chaque modèle\n",
    "for model in models:\n",
    "    # Entraîner le modèle\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédire les probabilités des classes\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "\n",
    "    # Calculer le score de précision moyenne\n",
    "    average_precision = average_precision_score(y_test, y_scores[:, 1])\n",
    "\n",
    "    # Calculer les valeurs de précision et de rappel pour différents seuils\n",
    "    disp = PrecisionRecallDisplay.from_estimator(model, X_test, y_test)\n",
    "\n",
    "    # Afficher le score de précision moyenne\n",
    "    print(f'Average precision-recall score for {model.__class__.__name__}: {average_precision}')\n",
    "\n",
    "# Afficher la courbe de précision-rappel\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LLlcv4hzvv1Y",
   "metadata": {
    "id": "LLlcv4hzvv1Y"
   },
   "outputs": [],
   "source": [
    "# Commentaire sur LogisticRegression\n",
    "\"\"\"\n",
    "LogisticRegression a un score moyen de précision-rappel de 0.383, ce qui est le plus élevé parmi tous les modèles.\n",
    "Cela indique qu'il a la meilleure performance en termes de compromis entre la précision et le rappel.\n",
    "\"\"\"\n",
    "\n",
    "# Commentaire sur DecisionTreeClassifier\n",
    "\"\"\"\n",
    "DecisionTreeClassifier a un score moyen de précision-rappel de 0.234, ce qui est relativement faible.\n",
    "Cela indique qu'il pourrait ne pas être très efficace pour équilibrer la précision et le rappel.\n",
    "\"\"\"\n",
    "\n",
    "# Commentaire sur RandomForestClassifier\n",
    "\"\"\"\n",
    "RandomForestClassifier a un score moyen de précision-rappel de 0.241, ce qui est légèrement supérieur à celui du DecisionTreeClassifier,\n",
    "mais il est toujours assez faible.\n",
    "\"\"\"\n",
    "\n",
    "# Commentaire sur SVC\n",
    "\"\"\"\n",
    "SVC a un score moyen de précision-rappel de 0.236, ce qui est similaire à celui du DecisionTreeClassifier et du RandomForestClassifier.\n",
    "\"\"\"\n",
    "\n",
    "# Commentaire sur MLPClassifier\n",
    "\"\"\"\n",
    "MLPClassifier a le score moyen de précision-rappel le plus faible parmi tous les modèles (0.222),\n",
    "ce qui indique qu'il a la pire performance en termes de compromis entre la précision et le rappel.\n",
    "\"\"\"\n",
    "\n",
    "# Résumé\n",
    "\"\"\"\n",
    "En résumé, le modèle LogisticRegression semble être le meilleur choix parmi ces modèles pour ce problème de classification spécifique,\n",
    "selon les scores moyens de précision-rappel. Cependant, il est important de noter que ces résultats sont spécifiques à l'ensemble de données\n",
    "sur lequel les modèles ont été formés et testés. Les performances peuvent varier avec différents ensembles de données.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82d54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
